"""
Ada Intelligence Engine
========================

The core brain of Ada - orchestrates all capabilities with
Newton's constraint verification for guaranteed reliability.

Why Ada is BETTER than ChatGPT:
1. Every response is VERIFIED before returning
2. Math is checked with SymPy
3. Logic is checked for contradictions
4. Facts are verified against constraints
5. Full audit trail of all decisions
"""

import hashlib
import time
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, Generator, List, Optional, Union

from .schema import (
    AdaConfig,
    AdaMode,
    AdaResponse,
    AgentResult,
    AgentStatus,
    CanvasDocument,
    CodeLanguage,
    CodeResult,
    Conversation,
    Message,
    MessageRole,
    MemoryType,
    ResearchReport,
    ResponseFormat,
    Source,
    SourceType,
)

# Import Newton's LLM constraint system
try:
    from newton.llm import (
        Claim,
        ClaimBatch,
        ConstraintEngine,
        Domain,
        DomainValidator,
        GenerationResult,
        LogicValidator,
        MathValidator,
        PhysicsValidator,
        PolicyValidator,
        ValidatorRegistry,
        ValidationResult,
    )
    HAS_LLM_VALIDATORS = True
except ImportError:
    HAS_LLM_VALIDATORS = False


class IntelligenceMode(Enum):
    """
    Intelligence processing mode.

    Similar to GPT-5.2 tiers but with Newton verification.
    """

    INSTANT = "instant"      # Fast, lightweight verification
    THINKING = "thinking"    # Step-by-step reasoning with full verification
    PRO = "pro"              # Maximum capability, deep verification


# =============================================================================
# LLM CLIENT ABSTRACTION
# =============================================================================

class LLMClient:
    """
    Abstract LLM client interface.

    Supports multiple backends: OpenAI, Anthropic, local models, etc.
    """

    def __init__(
        self,
        model: str = "gpt-4",
        api_key: Optional[str] = None,
        api_base_url: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096,
    ):
        self.model = model
        self.api_key = api_key
        self.api_base_url = api_base_url
        self.temperature = temperature
        self.max_tokens = max_tokens

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        messages: Optional[List[Dict[str, str]]] = None,
        **kwargs
    ) -> str:
        """
        Generate a response from the LLM.

        For now, returns a mock response. In production, this would
        call OpenAI, Anthropic, or other LLM APIs.
        """
        # This is where you'd integrate with actual LLM APIs
        # For demonstration, we'll use a smart mock that handles common queries

        prompt_lower = prompt.lower()

        # Math questions
        if "2 + 2" in prompt_lower or "2+2" in prompt_lower:
            return '{"claims": [{"text": "2 + 2 = 4", "domain": "math"}]}'
        if "what is" in prompt_lower and any(op in prompt_lower for op in ["+", "-", "*", "/", "ร", "รท"]):
            return '{"claims": [{"text": "Mathematical calculation result", "domain": "math"}]}'

        # General knowledge
        if "capital" in prompt_lower:
            return '{"claims": [{"text": "The answer depends on the country specified.", "domain": "unknown"}]}'

        # Greetings
        if any(greet in prompt_lower for greet in ["hello", "hi", "hey"]):
            return '{"claims": [{"text": "Hello! I am Ada, your intelligent assistant. How can I help you today?", "domain": "unknown"}]}'

        # Default response
        return '{"claims": [{"text": "I can help you with that. Let me think about it carefully.", "domain": "unknown"}]}'

    def stream(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> Generator[str, None, None]:
        """Stream responses token by token."""
        response = self.generate(prompt, system_prompt, **kwargs)
        for word in response.split():
            yield word + " "
            time.sleep(0.05)  # Simulate streaming delay


# =============================================================================
# ADA ENGINE
# =============================================================================

class AdaEngine:
    """
    The core Ada engine.

    Coordinates between:
    - LLM generation
    - Newton constraint verification
    - Memory system
    - Research system
    - Agent system
    - Canvas system
    """

    def __init__(self, config: Optional[AdaConfig] = None):
        """Initialize the Ada engine."""
        self.config = config or AdaConfig()
        self.llm = LLMClient(
            model=self.config.model_name,
            api_key=self.config.api_key,
            api_base_url=self.config.api_base_url,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
        )

        # Initialize validator registry
        self._setup_validators()

        # Initialize subsystems (lazy loaded)
        self._memory = None
        self._research = None
        self._agent = None
        self._canvas = None
        self._scheduler = None
        self._sandbox = None
        self._connectors = None

    def _setup_validators(self):
        """Set up Newton's constraint validators."""
        if not HAS_LLM_VALIDATORS:
            self.registry = None
            self.constraint_engine = None
            return

        validators = []

        if self.config.verify_math:
            validators.append(MathValidator())

        if self.config.verify_logic:
            validators.append(LogicValidator())

        if self.config.verify_physics:
            validators.append(PhysicsValidator())

        # Add policy validator with forbidden topics
        if self.config.forbidden_topics:
            validators.append(PolicyValidator(
                forbidden_phrases=self.config.forbidden_topics,
                required_phrases=self.config.required_disclaimers,
            ))

        self.registry = ValidatorRegistry(validators)
        self.constraint_engine = ConstraintEngine(
            registry=self.registry,
            llm=self.llm,
            max_retries=self.config.max_verification_retries,
        )

    @property
    def memory(self):
        """Lazy load memory system."""
        if self._memory is None:
            from .memory import MemoryStore
            self._memory = MemoryStore(
                capacity=self.config.memory_capacity,
                persistence=self.config.memory_persistence,
                storage_file=self.config.memory_file,
            )
        return self._memory

    @property
    def research(self):
        """Lazy load research system."""
        if self._research is None:
            from .research import DeepResearch
            self._research = DeepResearch(
                llm=self.llm,
                max_sources=self.config.max_research_sources,
                depth=self.config.research_depth,
            )
        return self._research

    @property
    def agent(self):
        """Lazy load agent system."""
        if self._agent is None:
            from .agent import AdaAgent
            self._agent = AdaAgent(
                engine=self,
                max_steps=self.config.agent_max_steps,
                timeout=self.config.agent_timeout_seconds,
                require_approval=self.config.agent_require_approval,
            )
        return self._agent

    @property
    def canvas(self):
        """Lazy load canvas system."""
        if self._canvas is None:
            from .canvas import Canvas
            self._canvas = Canvas(llm=self.llm)
        return self._canvas

    @property
    def scheduler(self):
        """Lazy load task scheduler."""
        if self._scheduler is None:
            from .tasks import TaskScheduler
            self._scheduler = TaskScheduler(engine=self)
        return self._scheduler

    @property
    def sandbox(self):
        """Lazy load code sandbox."""
        if self._sandbox is None:
            from .sandbox import CodeSandbox
            self._sandbox = CodeSandbox(
                timeout=self.config.sandbox_timeout_seconds,
                memory_limit_mb=self.config.sandbox_memory_limit_mb,
                allowed_languages=self.config.allowed_languages,
            )
        return self._sandbox

    def generate(
        self,
        prompt: str,
        mode: Optional[IntelligenceMode] = None,
        conversation: Optional[Conversation] = None,
        format: ResponseFormat = ResponseFormat.MARKDOWN,
        verify: bool = True,
    ) -> AdaResponse:
        """
        Generate a verified response.

        This is the core method that makes Ada BETTER than ChatGPT:
        1. Generate response from LLM
        2. Extract claims from response
        3. Validate each claim against appropriate validators
        4. If violations found, repair and retry
        5. Return only verified content

        Args:
            prompt: The user's input
            mode: Intelligence mode (instant, thinking, pro)
            conversation: Optional conversation context
            format: Desired output format
            verify: Whether to run verification (default True)

        Returns:
            AdaResponse with verified content
        """
        start_time = time.time()
        mode = mode or IntelligenceMode(self.config.default_mode.value)

        # Build context from conversation
        system_prompt = self._build_system_prompt(mode)
        messages = []
        if conversation:
            messages = [
                {"role": m.role.value, "content": m.content}
                for m in conversation.get_context_window()
            ]

        # Use constraint engine if available and verification enabled
        if verify and self.constraint_engine and self.config.verify_facts:
            result = self.constraint_engine.generate(prompt)

            response = AdaResponse(
                content=self._format_claims(result.claims, format),
                mode=AdaMode(mode.value),
                format=format,
                verified=result.status == "verified",
                verification_status=result.status,
                verification_iterations=result.iterations,
                verification_details={
                    "violations": [
                        {
                            "claim": v.get("claim", ""),
                            "domain": v.get("domain", ""),
                            "rule": v.get("rule", ""),
                            "message": v.get("message", ""),
                        }
                        for v in result.violations
                    ] if hasattr(result, "violations") else []
                },
                claims=[{"text": c, "verified": True} for c in result.claims],
                processing_time_ms=int((time.time() - start_time) * 1000),
            )
        else:
            # Direct generation without verification
            raw_response = self.llm.generate(
                prompt,
                system_prompt=system_prompt,
                messages=messages,
            )

            # Extract content from JSON if present
            content = self._extract_content(raw_response)

            response = AdaResponse(
                content=content,
                mode=AdaMode(mode.value),
                format=format,
                verified=False,
                verification_status="skipped",
                processing_time_ms=int((time.time() - start_time) * 1000),
            )

        # Update memory if enabled
        if self.config.enable_memory:
            self._update_memory(prompt, response)

        return response

    def _build_system_prompt(self, mode: IntelligenceMode) -> str:
        """Build system prompt based on mode."""
        base = """You are Ada, an advanced AI assistant built on Newton's constraint verification system.
You are MORE RELIABLE than ChatGPT because all your outputs are verified before returning.

Your capabilities include:
- Verified mathematical reasoning (SymPy-powered)
- Logical consistency checking
- Deep research with source verification
- Autonomous task execution
- Code generation and execution
- Document and app building

Always be helpful, accurate, and honest. If you're uncertain, say so."""

        mode_additions = {
            IntelligenceMode.INSTANT: "\n\nMode: INSTANT - Provide quick, accurate responses.",
            IntelligenceMode.THINKING: "\n\nMode: THINKING - Show your reasoning step by step. Think carefully before answering.",
            IntelligenceMode.PRO: "\n\nMode: PRO - Maximum capability. Provide comprehensive, expert-level responses.",
        }

        return base + mode_additions.get(mode, "")

    def _format_claims(self, claims: List[str], format: ResponseFormat) -> str:
        """Format verified claims into output."""
        if not claims:
            return "I couldn't generate a verified response for that query."

        if format == ResponseFormat.JSON:
            import json
            return json.dumps({"verified_claims": claims}, indent=2)
        elif format == ResponseFormat.MARKDOWN:
            if len(claims) == 1:
                return claims[0]
            return "\n".join(f"- {claim}" for claim in claims)
        else:
            return " ".join(claims)

    def _extract_content(self, raw_response: str) -> str:
        """Extract content from raw LLM response."""
        import json
        try:
            data = json.loads(raw_response)
            if "claims" in data:
                claims = [c.get("text", c) if isinstance(c, dict) else c for c in data["claims"]]
                return " ".join(claims)
            return raw_response
        except json.JSONDecodeError:
            return raw_response

    def _update_memory(self, prompt: str, response: AdaResponse):
        """Update memory with new information."""
        if response.verified and response.claims:
            for claim in response.claims:
                if isinstance(claim, dict) and claim.get("verified"):
                    self.memory.add(
                        key=claim.get("text", "")[:100],
                        value=claim.get("text", ""),
                        memory_type=MemoryType.FACT,
                        verified=True,
                        source="ada_generation",
                    )


# =============================================================================
# MAIN ADA CLASS
# =============================================================================

class Ada:
    """
    Ada - The Better ChatGPT.

    A comprehensive AI assistant with verified outputs.

    Features:
    - chat(): Simple conversation
    - research(): Deep web research
    - agent(): Autonomous task execution
    - canvas(): Document/code building
    - execute(): Code execution
    - schedule(): Task scheduling
    - remember()/recall(): Memory management

    Example:
        ada = Ada()

        # Simple chat
        response = ada.chat("What is 2 + 2?")
        print(response.content)  # "2 + 2 = 4" (verified!)

        # Deep research
        report = ada.research("Latest in quantum computing")
        print(report.summary)

        # Agent mode
        result = ada.agent("Find all Python files and count lines")
        print(result.summary)
    """

    def __init__(self, config: Optional[AdaConfig] = None):
        """
        Initialize Ada.

        Args:
            config: Optional configuration. Uses defaults if not provided.
        """
        self.config = config or AdaConfig()
        self.engine = AdaEngine(self.config)

        # Current conversation
        self._conversation: Optional[Conversation] = None

        # Track usage
        self._total_tokens = 0
        self._total_requests = 0

    # =========================================================================
    # CHAT - Core Conversation
    # =========================================================================

    def chat(
        self,
        message: str,
        mode: Optional[Union[str, AdaMode]] = None,
        conversation: Optional[Conversation] = None,
        stream: bool = False,
    ) -> AdaResponse:
        """
        Send a chat message and get a verified response.

        Args:
            message: Your message
            mode: Optional mode override (instant, thinking, pro)
            conversation: Optional conversation context
            stream: Whether to stream the response

        Returns:
            AdaResponse with verified content

        Example:
            >>> ada = Ada()
            >>> response = ada.chat("What is the capital of France?")
            >>> print(response.content)
            "Paris is the capital of France."
        """
        # Convert string mode to enum
        if isinstance(mode, str):
            mode = AdaMode(mode.lower())

        intel_mode = IntelligenceMode(mode.value) if mode else None

        # Use current conversation if none provided
        conv = conversation or self._conversation or self._new_conversation()

        # Add user message
        conv.add_user_message(message)

        # Generate response
        response = self.engine.generate(
            prompt=message,
            mode=intel_mode,
            conversation=conv,
            verify=self.config.verify_facts,
        )

        # Add assistant message
        conv.add_assistant_message(
            response.content,
            verified=response.verified,
            verification_details=response.verification_details,
        )

        # Update current conversation
        self._conversation = conv

        # Track usage
        self._total_requests += 1
        self._total_tokens += response.tokens_used

        return response

    def _new_conversation(self) -> Conversation:
        """Create a new conversation."""
        return Conversation(mode=self.config.default_mode)

    def new_conversation(self, title: Optional[str] = None) -> Conversation:
        """
        Start a new conversation.

        Args:
            title: Optional title for the conversation

        Returns:
            New Conversation instance
        """
        self._conversation = Conversation(
            title=title,
            mode=self.config.default_mode,
        )
        return self._conversation

    @property
    def conversation(self) -> Optional[Conversation]:
        """Get current conversation."""
        return self._conversation

    # =========================================================================
    # RESEARCH - Deep Web Research
    # =========================================================================

    def research(
        self,
        query: str,
        max_sources: Optional[int] = None,
        depth: Optional[int] = None,
    ) -> ResearchReport:
        """
        Perform deep research on a topic.

        Ada will:
        1. Search the web for relevant sources
        2. Extract and verify claims from each source
        3. Cross-reference information
        4. Generate a comprehensive, verified report

        Args:
            query: Research topic or question
            max_sources: Maximum sources to consult
            depth: How deep to follow links (1-5)

        Returns:
            ResearchReport with verified findings

        Example:
            >>> report = ada.research("Latest advances in quantum computing")
            >>> print(report.summary)
            >>> print(report.key_findings)
        """
        if not self.config.enable_research:
            raise RuntimeError("Research mode is disabled in configuration")

        return self.engine.research.investigate(
            query=query,
            max_sources=max_sources or self.config.max_research_sources,
            depth=depth or self.config.research_depth,
        )

    # =========================================================================
    # AGENT - Autonomous Task Execution
    # =========================================================================

    def agent(
        self,
        task: str,
        require_approval: Optional[bool] = None,
        max_steps: Optional[int] = None,
    ) -> AgentResult:
        """
        Execute a task autonomously.

        Ada will:
        1. Analyze the task and create a plan
        2. Execute each step, using tools as needed
        3. Verify results at each step
        4. Return comprehensive results

        Args:
            task: Description of what to accomplish
            require_approval: Whether to pause for human approval
            max_steps: Maximum execution steps

        Returns:
            AgentResult with execution details

        Example:
            >>> result = ada.agent("Find all TODO comments in the codebase")
            >>> print(result.summary)
            >>> print(result.output)
        """
        if not self.config.enable_agent:
            raise RuntimeError("Agent mode is disabled in configuration")

        return self.engine.agent.execute(
            task=task,
            require_approval=require_approval if require_approval is not None else self.config.agent_require_approval,
            max_steps=max_steps or self.config.agent_max_steps,
        )

    # =========================================================================
    # CANVAS - Document/Code Building
    # =========================================================================

    def canvas(
        self,
        instruction: str,
        document: Optional[CanvasDocument] = None,
    ) -> CanvasDocument:
        """
        Build or edit a document using Canvas.

        Ada will:
        1. Understand the instruction
        2. Create or modify the document
        3. Verify content accuracy
        4. Return the updated document

        Args:
            instruction: What to create or change
            document: Existing document to modify (optional)

        Returns:
            CanvasDocument with content

        Example:
            >>> doc = ada.canvas("Create a Python function to calculate fibonacci")
            >>> print(doc.content)
        """
        if not self.config.enable_canvas:
            raise RuntimeError("Canvas mode is disabled in configuration")

        return self.engine.canvas.process(
            instruction=instruction,
            document=document,
        )

    # =========================================================================
    # CODE EXECUTION - Sandboxed Code Running
    # =========================================================================

    def execute(
        self,
        code: str,
        language: Union[str, CodeLanguage] = CodeLanguage.PYTHON,
    ) -> CodeResult:
        """
        Execute code in a secure sandbox.

        Ada will:
        1. Validate the code for safety
        2. Execute in isolated environment
        3. Capture output and errors
        4. Return results

        Args:
            code: Code to execute
            language: Programming language

        Returns:
            CodeResult with execution output

        Example:
            >>> result = ada.execute("print(2 + 2)")
            >>> print(result.stdout)  # "4"
        """
        if not self.config.enable_code_execution:
            raise RuntimeError("Code execution is disabled in configuration")

        if isinstance(language, str):
            language = CodeLanguage(language.lower())

        return self.engine.sandbox.execute(code, language)

    # =========================================================================
    # TASKS - Scheduling
    # =========================================================================

    def schedule(
        self,
        prompt: str,
        name: str,
        frequency: str = "daily",
        **kwargs
    ):
        """
        Schedule a recurring task.

        Args:
            prompt: What Ada should do
            name: Task name
            frequency: How often (once, hourly, daily, weekly, monthly)

        Example:
            >>> ada.schedule(
            ...     "Summarize today's AI news",
            ...     name="Daily AI Summary",
            ...     frequency="daily"
            ... )
        """
        if not self.config.enable_tasks:
            raise RuntimeError("Task scheduling is disabled in configuration")

        return self.engine.scheduler.schedule(
            prompt=prompt,
            name=name,
            frequency=frequency,
            **kwargs
        )

    # =========================================================================
    # MEMORY - Remember and Recall
    # =========================================================================

    def remember(self, key: str, value: Any, verified: bool = False):
        """
        Store information in memory.

        Args:
            key: Memory key/label
            value: Information to remember
            verified: Whether the information is verified

        Example:
            >>> ada.remember("user_name", "Alice")
            >>> ada.remember("favorite_color", "blue", verified=True)
        """
        if not self.config.enable_memory:
            raise RuntimeError("Memory is disabled in configuration")

        return self.engine.memory.add(
            key=key,
            value=value,
            memory_type=MemoryType.FACT if verified else MemoryType.PREFERENCE,
            verified=verified,
        )

    def recall(self, query: str) -> List[Any]:
        """
        Search memory for information.

        Args:
            query: Search query

        Returns:
            List of matching memory entries

        Example:
            >>> memories = ada.recall("user preferences")
        """
        if not self.config.enable_memory:
            return []

        return self.engine.memory.search(query)

    def forget(self, key: str) -> bool:
        """
        Remove information from memory.

        Args:
            key: Memory key to remove

        Returns:
            True if removed, False if not found
        """
        if not self.config.enable_memory:
            return False

        return self.engine.memory.remove(key)

    # =========================================================================
    # UTILITY METHODS
    # =========================================================================

    def set_mode(self, mode: Union[str, AdaMode]):
        """Set the default intelligence mode."""
        if isinstance(mode, str):
            mode = AdaMode(mode.lower())
        self.config.default_mode = mode

    def get_stats(self) -> Dict[str, Any]:
        """Get usage statistics."""
        return {
            "total_requests": self._total_requests,
            "total_tokens": self._total_tokens,
            "memory_entries": len(self.engine.memory.entries) if self.config.enable_memory else 0,
            "conversations": 1 if self._conversation else 0,
        }

    def export_conversation(self, filepath: str):
        """Export current conversation to file."""
        if not self._conversation:
            raise ValueError("No active conversation to export")

        import json
        with open(filepath, "w") as f:
            json.dump(self._conversation.to_dict(), f, indent=2)

    def import_conversation(self, filepath: str):
        """Import conversation from file."""
        import json
        with open(filepath, "r") as f:
            data = json.load(f)
        self._conversation = Conversation.from_dict(data)

    def __repr__(self) -> str:
        return f"Ada(mode={self.config.default_mode.value}, verified={self.config.verify_facts})"
